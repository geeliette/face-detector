{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd4661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to your dataset\n",
    "train_dir = r\"/Users/julietteliao/Desktop/face-detector/deep/data/train_images\"    # folder containing training images\n",
    "test_dir  = r\"/Users/julietteliao/Desktop/face-detector/deep/data/test_images\"     # folder containing test images\n",
    "\n",
    "# Define transformations (convert to gray-scale, tensor, normalize)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),   # transforms to gray-scale (1 input channel)\n",
    "     transforms.Resize((92, 112)),  # match the original calculation\n",
    "     transforms.ToTensor(),    # transforms to Torch tensor (needed for PyTorch)\n",
    "     transforms.Normalize(mean=(0.5,), std=(0.5,))]) # subtracts mean (0.5) and divides by std (0.5) -> resulting values in (-1, +1)\n",
    "\n",
    "# Define two pytorch datasets (train/test) \n",
    "train_data = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_data  = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "valid_size = 0.2   # proportion of validation set (80% train, 20% validation)\n",
    "batch_size = 32    \n",
    "\n",
    "# Define randomly the indices of examples to use for training and for validation\n",
    "num_train = len(train_data)\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:], indices_train[:split_tv]\n",
    "\n",
    "# subset_size = 500  # number of images you want to use for quick test\n",
    "# train_new_idx = train_new_idx[:subset_size]\n",
    "# valid_idx = valid_idx[:int(subset_size*0.2)]  # keep 20% for validation\n",
    "\n",
    "\n",
    "\n",
    "# Define two \"samplers\" that will randomly pick examples from the training and validation set\n",
    "train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# Dataloaders (take care of loading the data from disk, batch by batch, during training)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=1)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "classes = ('noface','face')  # indicates that \"1\" means \"face\" and \"0\" non-face (only used for display)\n",
    "\n",
    "# Quick check: see number of examples\n",
    "print(f\"Training samples: {len(train_new_idx)}\")\n",
    "print(f\"Validation samples: {len(valid_idx)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Train samples: {len(train_data)}\")\n",
    "\n",
    "\n",
    "# Visualize a few images from the training dataset\n",
    "def show_images(loader, num_images=8):\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15,3))\n",
    "    for i in range(num_images):\n",
    "        img = images[i].squeeze().numpy()  # remove channel dimension\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(classes[labels[i]])\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_images(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a5a62-cd7a-4405-a693-47f3ad7b7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d87fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 23 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss funciton n stuff\n",
    "import torch.optim as optim\n",
    "\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()  # for 2-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f46084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "SAVE_BEST = True\n",
    "MODEL_PATH = \"model_best.pth\"\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "    \n",
    "    avg_valid_loss = valid_loss / len(valid_loader)\n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{N_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Valid Loss: {avg_valid_loss:.4f}\")\n",
    "    print(f\"  Valid Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    if SAVE_BEST:\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            print(f\"  New best model saved! (Acc: {accuracy:.2f}%)\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "print(f\"\\nTraining complete! Best accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df541774-1731-492b-8a03-2d47194d74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model_best.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        outputs = model(images) \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        total += labels.size(0)       \n",
    "        correct += (predictions == labels).sum().item() \n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce3899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, ops\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class DetectionConfig:\n",
    "    STEP_SIZE = 40\n",
    "    WINDOW_SIZE = (112, 92)\n",
    "    SCORE_THRESHOLD = 0.8\n",
    "    NMS_THRESHOLD = 0.15\n",
    "    PYRAMID_SCALE = 1.2\n",
    "    MIN_PYRAMID_SIZE = (112, 92)\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    TRANSFORM = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((92, 112)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ])\n",
    "\n",
    "def sliding_window(image, model, config):\n",
    "    detections = []\n",
    "    img_np = np.array(image)\n",
    "    H, W = img_np.shape[:2]\n",
    "    window_width, window_height = config.WINDOW_SIZE\n",
    "\n",
    "    for y in range(0, H - window_height + 1, config.STEP_SIZE):\n",
    "        for x in range(0, W - window_width + 1, config.STEP_SIZE):\n",
    "            patch = image.crop((x, y, x + window_width, y + window_height))\n",
    "            patch_tensor = config.TRANSFORM(patch).unsqueeze(0).to(config.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                output = model(patch_tensor)\n",
    "                prob = torch.softmax(output, dim=1)\n",
    "                score = prob[0, 1].item()\n",
    "            if score >= config.SCORE_THRESHOLD:\n",
    "                detections.append((x, y, x + window_width, y + window_height, score))\n",
    "    return detections\n",
    "\n",
    "def image_pyramid(image, config):\n",
    "    pyramid = [image]\n",
    "    current = image\n",
    "    while True:\n",
    "        w = int(current.size[0] / config.PYRAMID_SCALE)\n",
    "        h = int(current.size[1] / config.PYRAMID_SCALE)\n",
    "        if w < config.MIN_PYRAMID_SIZE[0] or h < config.MIN_PYRAMID_SIZE[1]:\n",
    "            break\n",
    "        current = current.resize((w, h), Image.BILINEAR)\n",
    "        pyramid.append(current)\n",
    "    return pyramid\n",
    "\n",
    "def detect_faces(image, model, config=None, verbose=True):\n",
    "    if config is None:\n",
    "        config = DetectionConfig()\n",
    "    model.eval()\n",
    "    if image.mode != 'L':\n",
    "        image = image.convert('L')\n",
    "\n",
    "    all_detections = []\n",
    "    pyramid = image_pyramid(image, config)\n",
    "    for scaled_image in pyramid:\n",
    "        scale_w = image.width / scaled_image.width\n",
    "        scale_h = image.height / scaled_image.height\n",
    "        detections = sliding_window(scaled_image, model, config)\n",
    "        for x1, y1, x2, y2, score in detections:\n",
    "            all_detections.append((\n",
    "                int(x1 * scale_w),\n",
    "                int(y1 * scale_h),\n",
    "                int(x2 * scale_w),\n",
    "                int(y2 * scale_h),\n",
    "                score\n",
    "            ))\n",
    "\n",
    "    if not all_detections:\n",
    "        return []\n",
    "\n",
    "    boxes = torch.tensor([[x1, y1, x2, y2] for x1, y1, x2, y2, score in all_detections],\n",
    "                         dtype=torch.float, device=config.DEVICE)\n",
    "    scores = torch.tensor([score for _, _, _, _, score in all_detections],\n",
    "                          dtype=torch.float, device=config.DEVICE)\n",
    "    keep = ops.nms(boxes, scores, iou_threshold=config.NMS_THRESHOLD)\n",
    "    final_boxes = boxes[keep]\n",
    "    final_scores = scores[keep]\n",
    "\n",
    "    return [(int(x1.item()), int(y1.item()), int(x2.item()), int(y2.item()), score.item()) \n",
    "            for (x1, y1, x2, y2), score in zip(final_boxes, final_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1e4f7-fb3f-417f-8602-05919243b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "fddb_root = r\"/Users/julietteliao/Desktop/face-detector/deep/data/detector_test_images/Dataset_FDDB/Dataset_FDDB/images\"\n",
    "fddb_labels = r\"/Users/julietteliao/Desktop/face-detector/deep/data/detector_test_images/Dataset_FDDB/Dataset_FDDB/label.txt\"\n",
    "nonface_root = r\"/Users/julietteliao/Desktop/face-detector/deep/data/test_images\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def parse_fddb_labels(label_file):\n",
    "    gt_boxes = {}\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        if line.startswith('#'):\n",
    "            img_path = line[2:].strip()\n",
    "            boxes = []\n",
    "            i += 1\n",
    "            while i < len(lines) and not lines[i].startswith('#'):\n",
    "                coords = lines[i].strip().split()\n",
    "                if len(coords) >= 4:\n",
    "                    x1, y1, x2, y2 = map(int, coords[:4])\n",
    "                    boxes.append((x1, y1, x2, y2))\n",
    "                i += 1\n",
    "            if boxes:\n",
    "                gt_boxes[img_path] = boxes\n",
    "        else:\n",
    "            i += 1\n",
    "    return gt_boxes\n",
    "\n",
    "print(\"Loading ground truth bounding boxes...\")\n",
    "ground_truth = parse_fddb_labels(fddb_labels)\n",
    "print(f\"Loaded {len(ground_truth)} images with ground truth boxes\")\n",
    "\n",
    "class FaceDetectionDataset(Dataset):\n",
    "    def __init__(self, face_root, nonface_root, ground_truth, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.ground_truth = ground_truth\n",
    "        \n",
    "        for year in os.listdir(face_root):\n",
    "            year_path = os.path.join(face_root, year)\n",
    "            if not os.path.isdir(year_path):\n",
    "                continue\n",
    "            for month in os.listdir(year_path):\n",
    "                month_path = os.path.join(year_path, month)\n",
    "                if not os.path.isdir(month_path):\n",
    "                    continue\n",
    "                for day in os.listdir(month_path):\n",
    "                    day_path = os.path.join(month_path, day)\n",
    "                    if not os.path.isdir(day_path):\n",
    "                        continue\n",
    "                    big_path = os.path.join(day_path, \"big\")\n",
    "                    if not os.path.isdir(big_path):\n",
    "                        continue\n",
    "                    for fname in os.listdir(big_path):\n",
    "                        if fname.lower().endswith(('.jpg', '.jpeg', '.png', '.pgm')):\n",
    "                            fpath = os.path.join(big_path, fname)\n",
    "                            rel_path = f\"{year}/{month}/{day}/big/{fname}\"\n",
    "                            if rel_path in self.ground_truth:\n",
    "                                self.samples.append((fpath, 1, rel_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fpath, label, rel_path = self.samples[idx]\n",
    "        image = Image.open(fpath).convert(\"L\")\n",
    "\n",
    "        gt_boxes = []\n",
    "        if rel_path and rel_path in self.ground_truth:\n",
    "            gt_boxes = self.ground_truth[rel_path]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label, gt_boxes, fpath\n",
    "\n",
    "test_dataset = FaceDetectionDataset(fddb_root, nonface_root, ground_truth, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Total test samples: {len(test_dataset)}\")\n",
    "faces = sum([1 for _, label, _ in test_dataset.samples])\n",
    "nonfaces = len(test_dataset) - faces\n",
    "print(f\"Face images: {faces}, Non-face images: {nonfaces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37387e24-3098-4288-b618-bfe18516a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box1, box2):\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "\n",
    "    x1_i = max(x1_1, x1_2)\n",
    "    y1_i = max(y1_1, y1_2)\n",
    "    x2_i = min(x2_1, x2_2)\n",
    "    y2_i = min(y2_1, y2_2)\n",
    "\n",
    "    if x2_i < x1_i or y2_i < y1_i:\n",
    "        return 0.0\n",
    "\n",
    "    intersection = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    union = area1 + area2 - intersection\n",
    "\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def match_detections_to_gt(detections, gt_boxes, iou_threshold=0.3):\n",
    "    if len(gt_boxes) == 0 and len(detections) == 0:\n",
    "        return 0, 0, 0\n",
    "    if len(gt_boxes) == 0:\n",
    "        return 0, len(detections), 0\n",
    "    if len(detections) == 0:\n",
    "        return 0, 0, len(gt_boxes)\n",
    "\n",
    "    gt_matched = [False] * len(gt_boxes)\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "\n",
    "    for det in detections:\n",
    "        det_box = (det[0], det[1], det[2], det[3])\n",
    "        matched_any = False\n",
    "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "            if compute_iou(det_box, gt_box) >= iou_threshold:\n",
    "                true_positives += 1\n",
    "                gt_matched[gt_idx] = True\n",
    "                matched_any = True\n",
    "        if not matched_any:\n",
    "            false_positives += 1\n",
    "\n",
    "    false_negatives = sum([1 for m in gt_matched if not m])\n",
    "    return true_positives, false_positives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608bdb2-aade-4540-b364-929a1c3378fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_images = 3000\n",
    "iou_threshold = 0.3\n",
    "\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "total_nonface_correct = 0\n",
    "total_nonface_wrong = 0\n",
    "processed = 0\n",
    "\n",
    "for images, labels, gt_boxes_list, fpaths in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "    if processed >= num_images:\n",
    "        break\n",
    "    \n",
    "    image_tensor = images[0]\n",
    "    label = labels[0].item()\n",
    "    \n",
    "    if len(gt_boxes_list) > 0 and len(gt_boxes_list[0]) > 0:\n",
    "        raw_boxes = gt_boxes_list[0]\n",
    "        gt_boxes = []\n",
    "        if isinstance(raw_boxes, (list, tuple)):\n",
    "            for i in range(0, len(raw_boxes), 4):\n",
    "                if i + 3 < len(raw_boxes):\n",
    "                    box = (int(raw_boxes[i]), int(raw_boxes[i+1]),\n",
    "                           int(raw_boxes[i+2]), int(raw_boxes[i+3]))\n",
    "                    gt_boxes.append(box)\n",
    "        else:\n",
    "            gt_boxes = [tuple(box) for box in raw_boxes]\n",
    "    else:\n",
    "        gt_boxes = []\n",
    "\n",
    "    fpath = fpaths[0]\n",
    "    image_pil = transforms.ToPILImage()(image_tensor.squeeze())\n",
    "    detections = detect_faces(image_pil, model)\n",
    "    \n",
    "    if label == 1:\n",
    "        tp, fp, fn = match_detections_to_gt(detections, gt_boxes, iou_threshold)\n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "    else:\n",
    "        if len(detections) == 0:\n",
    "            total_nonface_correct += 1\n",
    "        else:\n",
    "            total_nonface_wrong += 1\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    axes[0].imshow(image_pil, cmap='gray')\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis('off')\n",
    "    for gt_box in gt_boxes:\n",
    "        x1, y1, x2, y2 = gt_box\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                 linewidth=2, edgecolor='green', facecolor='none')\n",
    "        axes[0].add_patch(rect)\n",
    "\n",
    "    axes[1].imshow(image_pil, cmap='gray')\n",
    "    axes[1].set_title(f\"Detections: {len(detections)}\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    if label == 1:\n",
    "        for det in detections:\n",
    "            x1, y1, x2, y2, score = det\n",
    "            det_box = (x1, y1, x2, y2)\n",
    "            correct = any(compute_iou(det_box, gt_box) >= iou_threshold for gt_box in gt_boxes)\n",
    "            color = 'green' if correct else 'red'\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     linewidth=2, edgecolor=color, facecolor='none')\n",
    "            axes[1].add_patch(rect)\n",
    "            axes[1].text(x1, y1 - 5, f\"{score:.2f}\", color='yellow', fontsize=8)\n",
    "    else:\n",
    "        for x1, y1, x2, y2, score in detections:\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     linewidth=2, edgecolor='red', facecolor='none')\n",
    "            axes[1].add_patch(rect)\n",
    "            axes[1].text(x1, y1 - 5, f\"{score:.2f}\", color='yellow', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    processed += 1\n",
    "\n",
    "precision = total_tp / (total_tp + total_fp) * 100 if (total_tp + total_fp) > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) * 100 if (total_tp + total_fn) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETECTION EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Positives: {total_tp}\")\n",
    "print(f\"False Positives: {total_fp}\")\n",
    "print(f\"False Negatives: {total_fn}\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1-Score: {f1_score:.2f}%\")\n",
    "print(\"\\nNon-face images:\")\n",
    "print(f\"  Correct: {total_nonface_correct}\")\n",
    "print(f\"  Wrong: {total_nonface_wrong}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e46d3867-5e78-4690-aef8-e038d90f4d31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999a2bf-c1a1-4541-8173-5872d8593a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441e6e0-1fa8-4caa-aaf2-244be6b7d942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
